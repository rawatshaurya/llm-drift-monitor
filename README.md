# ðŸš¦ LLM Drift Monitor  
### Semantic â€¢ Structural â€¢ Safety â€¢ Cost Drift Monitoring for LLMs

A **production-style LLM behavior monitoring system** that answers the question:

> **â€œIs this model still behaving the way we expect?â€**

This project goes beyond accuracy to detect **semantic drift**, **verbosity drift**, **safety drift**, and **cost drift** in Large Language Models using statistical tests, embeddings, and controlled experiments.

---

## âœ¨ Key Features

- ðŸ” **Semantic Drift Detection**
  - Embedding centroid cosine distance on model responses
- ðŸ“ **Structural Drift Detection**
  - Response length distribution shift via Kolmogorovâ€“Smirnov test
- ðŸ›¡ï¸ **Safety Drift**
  - Refusal-rate changes across models or time
- ðŸ’° **Estimated Cost Drift**
  - Token + cost estimation from verbosity changes
- ðŸ§ª **Controlled Experiments**
  - Equal-sample A/B comparisons using `EXPERIMENT_ID`
- ðŸ“ˆ **Drift Over Time**
  - Rolling-window drift visualization
- ðŸš¦ **Auto Alerts**
  - PASS / WARN / ALERT badges with explainable reasons
- ðŸ“Š **Streamlit Dashboard**
  - Interactive, production-style monitoring UI

---

## ðŸ§± Tech Stack

- **LLM Runtime:** Ollama (local inference)
- **Models Tested:** LLaMA-3-8B, Qwen-2.5-7B
- **Embeddings:** sentence-transformers/all-MiniLM-L6-v2
- **Storage:** SQLite
- **Statistics:** SciPy (KS test)
- **Visualization:** Streamlit
- **Language:** Python 3.10+

---

## ðŸ“‚ Repository Structure
```
llm-drift-monitor/
â”œâ”€ dashboard/
â”‚ â””â”€ app.py
â”œâ”€ data/
â”‚ â””â”€ llm_logs.db # ignored by git
â”œâ”€ reports/
â”‚ â””â”€ .gitkeep
â”œâ”€ scripts/
â”‚ â”œâ”€ run_prompt.py
â”‚ â”œâ”€ run_daily_monitor.py
â”‚ â”œâ”€ run_model_compare.py
â”‚ â”œâ”€ check_counts.py
â”‚ â””â”€ inspect_experiments.py
â”œâ”€ src/
â”‚ â”œâ”€ drift/
â”‚ â”‚ â””â”€ detectors.py
â”‚ â”œâ”€ features/
â”‚ â”‚ â”œâ”€ embed.py
â”‚ â”‚ â””â”€ text_features.py
â”‚ â”œâ”€ llm/
â”‚ â”‚ â””â”€ ollama_client.py
â”‚ â””â”€ logging/
â”‚ â”œâ”€ logger.py
â”‚ â””â”€ schema.py
â”œâ”€ .gitignore
â”œâ”€ LICENSE
â”œâ”€ requirements.txt
â””â”€ README.md

```

---

## ðŸš€ Quickstart

### 1ï¸âƒ£ Create & activate virtual environment (Windows PowerShell)

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
### 2ï¸âƒ£ Install dependencies

pip install -r requirements.txt
### 3ï¸âƒ£ Start Ollama & pull models

ollama list
ollama pull llama3:8b
ollama pull qwen2.5:7b
### 4ï¸âƒ£ Log LLM interactions
This logs prompts, responses, embeddings, latency, length, and refusal flags.


python -m scripts.run_prompt
### 5ï¸âƒ£ Run CLI drift monitor

python -m scripts.run_daily_monitor
### 6ï¸âƒ£ Launch Streamlit dashboard

streamlit run dashboard/app.py

```
ðŸ§ª Controlled Model Switch Experiment (Equal Samples)
This performs a clean A/B test:

-Same prompts
-Same sample size
-Only model changes

```
$env:EXPERIMENT_ID="model_switch_equal_samples"

$env:OLLAMA_MODEL="llama3:8b"
1..31 | % { python -m scripts.run_prompt }   # â‰ˆ93 samples

$env:OLLAMA_MODEL="qwen2.5:7b"
1..31 | % { python -m scripts.run_prompt }

python -m scripts.check_counts
python -m scripts.run_model_compare

```
Expected outcome:

âœ… Mild semantic drift

âš ï¸ Strong verbosity / length drift

âš ï¸ Possible refusal-rate changes

ðŸ“Š Dashboard Overview
Tabs included:
-Drift Snapshot â€“ current baseline vs recent behavior
-Drift Over Time â€“ rolling-window semantic & length drift
-Model Compare â€“ equal-sample A/B comparison
-Recent Logs â€“ raw interaction inspection

Auto Alert System
Each comparison is labeled:

-ðŸŸ¢ PASS â€“ normal variation
-ðŸŸ¡ WARN â€“ moderate drift detected
-ðŸ”´ ALERT â€“ statistically significant behavior change

Alerts are triggered using:

-semantic drift thresholds
-KS statistic + p-value
-refusal-rate increase
-estimated cost increase

ðŸ“ Metrics & Interpretation
Semantic Drift (Cosine Distance)
Range	Meaning
0.00 â€“ 0.03	Tiny
0.03 â€“ 0.08	Mild
0.08 â€“ 0.15	Moderate
> 0.15	High

Structural Drift (KS Test)
-KS statistic â†‘ â†’ larger distribution shift
-p < 0.01 â†’ statistically significant drift

Why this matters
Even when semantic content stays stable, drift in:

-verbosity
-latency
-refusal behavior
-token cost

can silently degrade UX, increase spend, or break workflows.

ðŸ§  Key Insight Demonstrated
Switching from LLaMA-3-8B to Qwen-2.5-7B caused major verbosity and refusal-rate drift while semantic meaning remained largely stable â€” showing why accuracy alone is insufficient for LLM evaluation.

ðŸ”’ Notes
-data/llm_logs.db is intentionally not committed
-Use EXPERIMENT_ID to keep experiments isolated and reproducible
-Cost estimates are approximate (token â‰ˆ 1.33 Ã— words)

